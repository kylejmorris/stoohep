from collections import defaultdict
import openai
import time
import random
import os
import traceback
import jsonlines
import hashlib
from config import config    
try:
    from api_key import openai_key
except:
    raise Exception("Create an api_key.py file with a dict including your OpenAI API key.")
from concurrent.futures import ThreadPoolExecutor as ThreadPool
from collections import Counter

MAX_BATCH = 5  # most number to query at once
MAX_TOKENS = 1024

def set_openai_key():
    """Sets OpenAI key."""
    if 'api_type' in openai_key and openai_key['api_type'] == 'azure':
        openai.api_type = "azure"
        openai.api_version = "2023-03-15-preview"
        openai.api_base = openai_key['api_base']
    else:
        if 'organization' in openai_key:
            openai.organization = openai_key['organization']
    openai.api_key = openai_key['api_key']

if openai.api_type == 'azure':
    # These will depend on how you deployed the OpenAI resource on Azure
    CHAT_GPT_35 = "gpt-35-turbo"
    CHAT_GPT_4 = "gpt-4"
else:
    CHAT_GPT_35 = "gpt-3.5-turbo-0301"
    CHAT_GPT_4 = "gpt-4-0314"
default_engine = CHAT_GPT_4

cache_counter = defaultdict(int)
set_openai_key()

def write_to_usage_log(role, message, n, t):
    # Write to usage log - using jsonlines for easy appending
    try:
        with jsonlines.open("usage_log.jsonl", "a") as writer:
            writer.write([
                time.time(),
                {
                    "role": role,
                    "message": message,
                    "n": n,
                    "t": t,
                }
            ])
    except Exception as e:
        print("Failed to write to usage log with exception", e)
        print("Traceback:", traceback.format_exc())

class LanguageModel:
    def __init__(self, budget):
        """
        Initializes the language model.

        Args:
            role (str): The role of the language model.
        """
        self.next_request_time = time.time()
        self.rate_limit = 0.5  # Initial requests per second - will automatically lower if not feasible
        self.min_rate_limit = 15 / 60  # Can't be lower than this
        self.global_timeout = 1024
        if not os.path.exists("cache"):
            os.mkdir("cache")
        self.cache_counter = cache_counter
        self.use_cache = config['use_language_model_cache']
        self.times_used = 0
        self.budget = budget
        self.max_responses_per_call = config['max_responses_per_call']

    def prompt(self, expertise, message, n_responses=1, temperature=0.7):
        """
        Generates a response to a message.

        Args:
            expertise (str): The expertise of the language model.
            message (str): The message to respond to.
            n (int): The number of responses to generate.
            t (float): The temperature of the language model.

        Returns:
            responses (list of str): The responses generated by the language model.
        """
        # Make sure the message is a string
        role = expertise
        assert isinstance(message, str)
        assert isinstance(role, str)
        assert n_responses <= self.max_responses_per_call
        self.times_used += 1
        if self.times_used > self.budget:
            raise Exception("Error: You have exceeded your call budget.")
        n, t = n_responses, temperature
        cache_key = str((message, n, t))
        cache_file = f"{hashlib.sha256(cache_key.encode()).hexdigest()}.jsonl"
        write_to_usage_log(role, message, n, t)
        if self.use_cache:
            if cache_file in os.listdir("cache"):
                self.cache_counter[cache_file] += 1
                # Open the file as a jsonl file
                try:
                    with jsonlines.open(f"cache/{cache_file}") as reader:
                        messages = [message for message in reader]
                except Exception as e:
                    print("Failed to read from cache with exception", e)
                    print("Traceback:", traceback.format_exc())
                    messages = []
                    # Remove the cache file
                    try:
                        os.remove(f"cache/{cache_file}")
                    except Exception as e:
                        print("Failed to remove cache file with exception", e)
                        print("Traceback:", traceback.format_exc())
                if len(messages) > self.cache_counter[cache_file]:
                    return messages[self.cache_counter[cache_file] - 1]
        max_n = MAX_BATCH
        max_tokens = MAX_TOKENS
        next_request_time = self.next_request_time
        rate_limit = self.rate_limit
        global_timeout = self.global_timeout
        min_rate_limit = self.min_rate_limit
        engine = default_engine
        system = role
        user = [message]
        n_max_messages = 8
        temperature = t

        # Prompt the model
        remaining = n
        remaining_to_submit = n
        error_count = 0
        while time.time() < next_request_time:
            time.sleep(random.random() * 0.01)

        # Tell everyone to wait for one rate limit iteration per call
        next_request_time = time.time() + (n / max_n) / rate_limit
        messages=[{"role": "system", "content": system}]
        for u_id, u in enumerate(user):
            role = "user" if u_id % 2 == 0 else "assistant"
            messages.append({"role": role, "content": u})
        if len(messages) > n_max_messages:
            if n_max_messages == 1:
                raise Exception("Error: This will exclude the system prompt.")
            else:
                messages = messages[:n_max_messages // 2] + messages[-n_max_messages // 2:]
        # Select a backend at random
        result = []
        print(f"Querying OpenAI API with messages... {remaining} left")

        def query_api(cur_n):
            set_openai_key()
            if openai.api_type == "azure":
                result.extend(openai.ChatCompletion.create(
                    engine=engine, messages=messages, n=cur_n, temperature=temperature,
                    max_tokens=max_tokens, timeout=global_timeout * cur_n
                ).choices)
            else:
                result.extend(openai.ChatCompletion.create(
                    model=engine, messages=messages, n=cur_n, temperature=temperature,
                    max_tokens=max_tokens, timeout=global_timeout * cur_n
                ).choices)

        threadpool = ThreadPool(max_workers=16)
        threadpool_futures = []
        # Submit the requests in parallel
        while remaining_to_submit > 0:
            cur_n = min(max_n, remaining_to_submit)
            threadpool_futures.append((cur_n, threadpool.submit(query_api, cur_n)))
            remaining_to_submit -= cur_n
            time.sleep(rate_limit)

        # Wait for the requests to finish. If they fail, retry them.
        while remaining > 0:
            if error_count > 10:
                break
            new_threadpool_futures = []
            for future_n, future in threadpool_futures:
                try:
                    future.result()
                    remaining -= future_n
                    rate_limit *= 1.01
                    print("Success! Queried OpenAI API with", future_n, "messages.")
                except Exception as e:
                    if "https://aka.ms/oai/quotaincrease" not in str(e) and "Rate limit reached for" not in str(e):
                        print("Error while querying OpenAI API. Retrying...", e)
                        error_count += 1
                    else:
                        rate_limit = max(rate_limit * 0.9, min_rate_limit)
                    new_threadpool_futures.append((future_n, threadpool.submit(query_api, future_n,)))
            threadpool_futures = new_threadpool_futures

        if remaining == 0:
            if "improve_algorithm" in message:
                write_time = time.time()
                save_folder = f"creativity/{int(write_time)}"
                os.makedirs(save_folder, exist_ok=True)
                with open(f"{save_folder}/message.txt", "w") as writer:
                    writer.write(message)
                for res_idx, res in enumerate(result):
                    with open(f"{save_folder}/response_{res_idx}.txt", "w") as writer:
                        writer.write(res.message.content)
        # Convert the response to a list of strings (or one string if n=1)
        results = [choice.message.content for choice in result]
        if cache_file not in os.listdir("cache"):
            with jsonlines.open(f"cache/{cache_file}", "w") as writer:
                writer.write(results)
        else:
            try:
                with jsonlines.open(f"cache/{cache_file}", "a") as writer:
                    writer.write(results)
            except Exception as e:
                print("Failed to write to existing cache with exception", e)
                print("Traceback:", traceback.format_exc())
        if self.use_cache:
            self.cache_counter[cache_file] += 1
        return results

    def batch_prompt(self, expertise, message_batch, temperature):
        """
        Generates a response to a message.

        Args:
            expertise (str): The expertise of the language model.
            message (str): The message to respond to.
            n (int): The number of responses to generate.
            t (float): The temperature of the language model.

        Returns:
            responses (list of str): The responses generated by the language model.
        """
        # Make sure the message is a string
        role = expertise
        try:
            message_batch = list(message_batch)
        except:
            return []
        assert len(message_batch) <= self.max_responses_per_call
        assert isinstance(message_batch[0], str)
        assert isinstance(role, str)
        self.times_used += 1
        if self.times_used > self.budget:
            raise Exception("Error: You have exceeded your call budget.")
        n, t = 1, temperature
        cache_key = str((str(message_batch), n, t))
        cache_file = f"{hashlib.sha256(cache_key.encode()).hexdigest()}.jsonl"
        write_to_usage_log(role, str(message_batch), n, t)
        if self.use_cache:
            if cache_file in os.listdir("cache"):
                self.cache_counter[cache_file] += 1
                # Open the file as a jsonl file
                try:
                    with jsonlines.open(f"cache/{cache_file}") as reader:
                        messages = [message for message in reader]
                except Exception as e:
                    print("Failed to read from cache with exception", e)
                    print("Traceback:", traceback.format_exc())
                    messages = []
                    # Remove the cache file
                    try:
                        os.remove(f"cache/{cache_file}")
                    except Exception as e:
                        print("Failed to remove cache file with exception", e)
                        print("Traceback:", traceback.format_exc())
                if len(message_batch) > self.cache_counter[cache_file]:
                    return messages[self.cache_counter[cache_file] - 1]
        max_n = MAX_BATCH
        max_tokens = MAX_TOKENS
        next_request_time = self.next_request_time
        rate_limit = self.rate_limit
        global_timeout = self.global_timeout
        min_rate_limit = self.min_rate_limit
        engine = default_engine
        system = role
        n_max_messages = 8
        temperature = t

        # Prompt the model
        error_count = 0
        while time.time() < next_request_time:
            time.sleep(random.random() * 0.01)

        # Select a backend at random
        result = []
        print(f"Querying OpenAI API with messages... {len(message_batch)} left")

        def query_api(user, n_responses=1):
            # Tell everyone to wait for one rate limit iteration per call
            next_request_time = time.time() + (n / max_n) / rate_limit
            messages=[{"role": "system", "content": system}]
            for u_id, u in enumerate(user):
                role = "user" if u_id % 2 == 0 else "assistant"
                messages.append({"role": role, "content": u})
            if len(messages) > n_max_messages:
                if n_max_messages == 1:
                    raise Exception("Error: This will exclude the system prompt.")
                else:
                    messages = messages[:n_max_messages // 2] + messages[-n_max_messages // 2:]

            set_openai_key()
            if openai.api_type == "azure":
                result.extend(openai.ChatCompletion.create(
                    engine=engine, messages=messages, n=n_responses, temperature=temperature,
                    max_tokens=max_tokens, timeout=global_timeout
                ).choices)
            else:
                result.extend(openai.ChatCompletion.create(
                    model=engine, messages=messages, n=n_responses, temperature=temperature,
                    max_tokens=max_tokens, timeout=global_timeout
                ).choices)

        threadpool = ThreadPool(max_workers=16)
        threadpool_futures = []
        # Submit the requests in parallel - group them together to save on time
        message_batch_counts = Counter(message_batch)
        for message, count in message_batch_counts.items():
            threadpool_futures.append((message, threadpool.submit(query_api, [message], count)))
            time.sleep(rate_limit)

        # Wait for the requests to finish. If they fail, retry them.
        while len(threadpool_futures) > 0:
            if error_count > 10:
                break
            new_threadpool_futures = []
            for future_message, future in threadpool_futures:
                try:
                    future.result()
                    rate_limit *= 1.01
                    print("Success! Queried OpenAI API")
                except Exception as e:
                    if "https://aka.ms/oai/quotaincrease" not in str(e) and "Rate limit reached for" not in str(e):
                        print("Error while querying OpenAI API. Retrying...", e)
                        error_count += 1
                    else:
                        rate_limit = max(rate_limit * 0.9, min_rate_limit)
                    new_threadpool_futures.append((future_message, threadpool.submit(query_api, [future_message],)))
            threadpool_futures = new_threadpool_futures

        write_time = time.time()
        save_folder = f"creativity/{int(write_time)}"
        for res_idx, (message, cur_result) in enumerate(zip(message_batch, result)):
            if "improve_algorithm" in message:
                os.makedirs(save_folder, exist_ok=True)
                with open(f"{save_folder}/message.txt", "w") as writer:
                    writer.write(message)
                with open(f"{save_folder}/response_{res_idx}.txt", "w") as writer:
                    writer.write(cur_result.message.content)
        # Convert the response to a list of strings (or one string if n=1)
        results = [choice.message.content for choice in result]
        if cache_file not in os.listdir("cache"):
            with jsonlines.open(f"cache/{cache_file}", "w") as writer:
                writer.write(results)
        else:
            try:
                with jsonlines.open(f"cache/{cache_file}", "a") as writer:
                    writer.write(results)
            except Exception as e:
                print("Failed to write to existing cache with exception", e)
                print("Traceback:", traceback.format_exc())
        if self.use_cache:
            self.cache_counter[cache_file] += 1
        return results

def test_lm():
    global MAX_TOKENS, default_engine
    MAX_TOKENS = 20
    config['use_language_model_cache'] = True
    default_engine = CHAT_GPT_35
    lm = LanguageModel(100)
    print(lm.prompt("You respond in three word answers.", "What's a great day?", 6, 0.2))
    print(lm.batch_prompt("You respond in three word answers.", [f"what is {i} + {i}?" for i in range(6)], 0.7))

if __name__ == "__main__":
    test_lm()